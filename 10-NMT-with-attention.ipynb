{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acfd243a-a340-4366-bfc5-6b6407289776",
   "metadata": {},
   "source": [
    "# Neural Machine Translation with Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cedff504-13ed-47bd-a245-524126d9fff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-03 09:58:49.261160: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-03 09:58:49.261188: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-03 09:58:49.262052: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import unicodedata\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "tf.config.experimental.set_memory_growth(tf.config.experimental.list_physical_devices('GPU')[0], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8901201-a302-4cfb-8401-bb26ea48c1a7",
   "metadata": {},
   "source": [
    "- Mark, Nonspacing (Mn): In Unicode, characters can have different categories assigned to them based on their properties. \"Mark, Nonspacing\" (Mn) refers to a category of characters that are typically diacritics or accents that are intended to be combined with other characters but don't take up space by themselves when displayed. For example, in the word \"café,\" the accent mark over the letter \"e\" is a diacritic that falls into the \"Mark, Nonspacing\" category in Unicode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8f14046-6612-4646-98e1-3405240f4a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicode_to_ascii(w):\n",
    "    return ''.join([x for x in unicodedata.normalize('NFD', w) if unicodedata.category(x) != 'Mn'])\n",
    " \n",
    "\n",
    "def preprocess_sentence(w):\n",
    "    w = unicode_to_ascii(w.lower().strip())\n",
    "    w = re.sub('([?!.(),¿])', r' \\1', w)\n",
    "    w = re.sub('\\s{2,}', ' ', w)\n",
    "    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w) # This replaces everything with space\n",
    "    w = w.rstrip().strip()\n",
    "    return '<start> ' + w + ' <end>' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34e10a80-0ce8-493c-885e-1d130d3dec5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(file_path, num_examples=None):\n",
    "    spa_en = []\n",
    "    cnt = 0\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file.readlines():\n",
    "            if num_examples is not None and cnt >= num_examples: break\n",
    "            line = line.split('\\t')[:2]\n",
    "            spa_en.append([preprocess_sentence(line[0]), preprocess_sentence(line[1])])\n",
    "            cnt += 1\n",
    "    return zip(*spa_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51c5dd0a-6903-4dbc-b47b-36893246ab6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> one day , i woke up to find that god had put hair on my face . i shaved it off . the next day , i found that god had put it back on my face , so i shaved it off again . on the third day , when i found that god had put hair back on my face again , i decided to let god have his way . that s why i have a beard . <end>\n",
      "<start> un dia , me desperte y vi que dios me habia puesto pelo en la cara . me lo afeite . al dia siguiente , vi que dios me lo habia vuelto a poner en la cara , asi que me lo afeite otra vez . al tercer dia , cuando vi que dios me habia puesto pelo en la cara de nuevo , decidi que dios se saliera con la suya . por eso tengo barba . <end>\n"
     ]
    }
   ],
   "source": [
    "FILE_PATH =  './temp/spa-eng/spa.txt'\n",
    "en, sp = create_dataset(FILE_PATH)\n",
    "print(en[-1])\n",
    "print(sp[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03487782-1541-43a2-8056-c1d43dd9fef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_length(tensor):\n",
    "    return max(len(t) for t in tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a0f5aaf-a909-49d6-b05d-ae5b448509f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(tensor):\n",
    "    lang_tokenizer = Tokenizer(filters='')\n",
    "    lang_tokenizer.fit_on_texts(tensor)\n",
    "    tensor = lang_tokenizer.texts_to_sequences(tensor)\n",
    "    tensor = pad_sequences(tensor, padding='post')\n",
    "    return tensor, lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2dbdb8c2-6a35-4f8f-8017-11b6f38467d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(file_name, num_examples=None):\n",
    "    src_lang, targ_lang = create_dataset(file_name, num_examples)\n",
    "    src_tensor, src_tokenizer = tokenize(src_lang)\n",
    "    targ_tensor, targ_tokenizer = tokenize(targ_lang)\n",
    "    return src_tensor, targ_tensor, src_tokenizer, targ_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2846bc33-d935-448c-895a-4af2d90fa870",
   "metadata": {},
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e50ea4ad-f6e1-4d7b-9db9-241d80de8ec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum length of English sentence is 15\n",
      "Maximum length for spanish sentence is 22\n"
     ]
    }
   ],
   "source": [
    "NUM_SAMPLES = 100000\n",
    "src_tensor, targ_tensor, src_tokenizer, targ_tokenizer = load_dataset(FILE_PATH, num_examples=NUM_SAMPLES)\n",
    "\n",
    "MAX_SRC_LENGTH, MAX_TARGET_LENGTH = max_length(src_tensor), max_length(targ_tensor)\n",
    "print(f'Maximum length of English sentence is {MAX_SRC_LENGTH}\\nMaximum length for spanish sentence is {MAX_TARGET_LENGTH}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b4a43bb-4a19-4f5d-b7ed-9e51e815588a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Train shape: (80000, 15)\n",
      "Source Validation shape: (20000, 15)\n",
      "Target Train shape: (80000, 22)\n",
      "Target Validation shape: (20000, 22)\n"
     ]
    }
   ],
   "source": [
    "src_train, src_val, targ_train, targ_val = train_test_split(src_tensor, targ_tensor, test_size=0.2)\n",
    "\n",
    "print('Source Train shape:', src_train.shape)\n",
    "print('Source Validation shape:', src_val.shape)\n",
    "print('Target Train shape:', targ_train.shape)\n",
    "print('Target Validation shape:', targ_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33a9582c-bd34-4129-a513-71044d68586e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ids_to_text(ids, lang):\n",
    "    return ' '.join(lang.index_word[i] for i in ids if i != 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f774bc3c-f9fe-453f-87ff-67edda0dbb0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Example (English)\n",
      "<start> can he play the guitar ? <end>\n",
      "Training Example (Spanish)\n",
      "<start> ¿puede el tocar la guitarra ? <end>\n"
     ]
    }
   ],
   "source": [
    "i = 3000\n",
    "print('Training Example (English)')\n",
    "print(ids_to_text(src_train[i], src_tokenizer))\n",
    "\n",
    "print('Training Example (Spanish)')\n",
    "print(ids_to_text(targ_train[i], targ_tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523b3e09-9048-4376-8906-1b4b337dd736",
   "metadata": {},
   "source": [
    "## Create the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4110cd3-1457-44c1-a033-f28de1faa3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "ENC_UNITS = 512\n",
    "DEC_UNITS = 1024\n",
    "EMBEDDING_DIM = 256\n",
    "VOCAB_SRC_SIZE = len(src_tokenizer.index_word) + 1\n",
    "VOCAB_TARG_SIZE = len(targ_tokenizer.index_word) + 1\n",
    "BATCH_SIZE = 64\n",
    "STEPS_PER_EPOCH = len(src_train) // BATCH_SIZE\n",
    "BUFFER_SIZE = len(src_train)\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((src_train, targ_train))\n",
    "train_ds = train_ds.shuffle(BUFFER_SIZE)\n",
    "train_ds = train_ds.batch(BATCH_SIZE, num_parallel_calls=AUTOTUNE, drop_remainder=True)\n",
    "train_ds = train_ds.prefetch(AUTOTUNE)\n",
    "\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((src_val, targ_val))\n",
    "test_ds = test_ds.batch(BATCH_SIZE, num_parallel_calls=AUTOTUNE, drop_remainder=True)\n",
    "test_ds = test_ds.prefetch(AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "313f4cb1-d29a-4af1-84de-fd3624b5d9a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Train Source: (64, 15)\n",
      "Example Train Target: (64, 22)\n"
     ]
    }
   ],
   "source": [
    "example_train_source, example_train_target = next(iter(train_ds))\n",
    "print('Example Train Source:', example_train_source.shape)\n",
    "print('Example Train Target:', example_train_target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04993e78-dd7f-4dc5-906f-21e977109795",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b41e91a-aadb-4dc7-af51-dedb7ac15247",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Layer, Dense, Activation, GRU, Bidirectional, Embedding\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "56262576-2137-4579-bc65-05e9c684648e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_size):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.enc_units = enc_units\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.embedding = Embedding(input_dim=vocab_size, output_dim=embedding_dim)\n",
    "        \n",
    "        self.gru = Bidirectional(GRU(\n",
    "            units=self.enc_units,\n",
    "            return_sequences=True, \n",
    "            return_state=True\n",
    "        ))\n",
    "\n",
    "        self.reset_hidden_states()\n",
    "\n",
    "    def call(self, inputs, hidden=None):\n",
    "        if hidden is not None:\n",
    "            self._hidden = hidden[0]\n",
    "            self._backward_hidden = hidden[1]\n",
    "            \n",
    "        embedding = self.embedding(inputs)\n",
    "        sequences, h, bh = self.gru(embedding, initial_state=(self._hidden, self._backward_hidden))\n",
    "        return sequences, (h, bh)\n",
    "\n",
    "    def reset_hidden_states(self):\n",
    "        self._hidden = tf.zeros((self.batch_size, self.enc_units))\n",
    "        self._backward_hidden = tf.zeros((self.batch_size, self.enc_units))\n",
    "\n",
    "    \n",
    "    def get_hidden_states(self):\n",
    "        return self._hidden, self._backward_hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b8debedc-1fa5-41f4-8d5c-4a7a97fbda67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output sequence shape: (64, 15, 1024)\n",
      "State shape for both forward and backward: (64, 512)\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(VOCAB_SRC_SIZE, EMBEDDING_DIM, ENC_UNITS, BATCH_SIZE)\n",
    "sample_sequence, sample_hidden = encoder(example_train_source)\n",
    "print('Output sequence shape:', sample_sequence.shape)\n",
    "print('State shape for both forward and backward:', sample_hidden[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c09f28c9-5b54-480a-b654-3b24e4278e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.Wa = Dense(units)\n",
    "        self.Ua = Dense(units)\n",
    "        self.V = Dense(1)\n",
    "\n",
    "    def call(self, query, values):\n",
    "        \"\"\"\n",
    "        args:\n",
    "            query is the previous state of the decoder i.e. s_{i-1}, the shape is (batches, decoder_units)\n",
    "            values are the output sequences from the decoder, the shape if (batches, max_src_length, encoder_units)\n",
    "        returns:\n",
    "        \"\"\"\n",
    "        # Let's add time dimension to query to have the shape (batches, 1, units)\n",
    "        query_with_time = tf.expand_dims(query, axis=1)\n",
    "\n",
    "        # Given Wa, the query, it computes a mapping with the shape of (batches, 1, units) \n",
    "        # Given Ua, the values, it computes a mapping with the shape of (batches, max_src_length, units)\n",
    "        # Note that adding these two is possible only when the result of Wa is broadcasted to have (batches, max_src_length, units)\n",
    "        # Give the resulting (batches, max_src_length, units) to V, it will compute scores with the shape of (batches, max_src_length, 1)\n",
    "        scores = self.V(tf.nn.tanh(self.Wa(query_with_time) + self.Ua(values)))\n",
    "\n",
    "        # attention_weights has the shape (batches, max_src_length, 1)\n",
    "        attention_weights = tf.nn.softmax(scores, axis=1)\n",
    "\n",
    "        # attention weights are broadcasted to be able to multiply it by values with shape of (batches, max_src_length, encoder_units)\n",
    "        context_vector = attention_weights * values\n",
    "        # context vector will be expected annotation over all the annotations with the shape (batches, encoder_units)\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9c983004-84f5-4141-86ce-53bcfe2fd378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context vector's shape is (64, 1024)\n",
      "Attention_weights' shape is (64, 15, 1)\n"
     ]
    }
   ],
   "source": [
    "attention = BahdanauAttention(units=10)\n",
    "s_prev = tf.zeros((BATCH_SIZE, DEC_UNITS))\n",
    "context_vector, attention_weights = attention(s_prev, sample_sequence)\n",
    "print('Context vector\\'s shape is', context_vector.shape)\n",
    "print('Attention_weights\\' shape is', attention_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "59556198-0d72-4304-8f1e-6bff9ad40f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.dec_units = dec_units\n",
    "        self.batch_size = batch_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        self.embedding = Embedding(input_dim=vocab_size, output_dim=embedding_dim)\n",
    "        self.gru = GRU(units=dec_units, return_state=True, return_sequences=True)\n",
    "        self.attention = BahdanauAttention(dec_units)\n",
    "        self.fc = Dense(units=vocab_size)\n",
    "\n",
    "        self.reset_states()\n",
    "    def call(self, inputs, enc_output, hidden_state=None):\n",
    "        # note that inputs have the shape (batch_size, 1)\n",
    "        if hidden_state is not None:\n",
    "            self._hidden_state = hidden_state\n",
    "\n",
    "        context_vector, attention_weights = self.attention(self._hidden_state, enc_output)\n",
    "\n",
    "        # embeddings will have the shape (batch_size, 1, embedding_dim)\n",
    "        embeddings = self.embedding(inputs)\n",
    "        \n",
    "        # Add time-step dimension for the context_vector to have (batch, 1, enc_units)\n",
    "        context_vector = tf.expand_dims(context_vector, 1)\n",
    "        \n",
    "        # Concat context and embeddings\n",
    "        # Should result in (batch, 1, enc_units + embedding_dim)\n",
    "        x = tf.concat([context_vector, embeddings], axis=-1)\n",
    "\n",
    "        # outputs shape will be (batch, 1, dec_units)\n",
    "        outputs, states = self.gru(x)\n",
    "\n",
    "        # outputs shape will be (batch * 1, dec_units) \n",
    "        outputs = tf.reshape(outputs, (-1, outputs.shape[2]))\n",
    "\n",
    "        # resulting outputs will have the shape (batch, target_vocab_size)\n",
    "        outputs = self.fc(outputs)\n",
    "        return outputs, states, attention_weights\n",
    "\n",
    "    def reset_states(self):\n",
    "        self._hidden_state = tf.zeros((self.batch_size, self.dec_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2da5d9a3-fba1-4520-a787-157c8e2c3533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder outputs shape is (64, 20468)\n"
     ]
    }
   ],
   "source": [
    "decoder = Decoder(VOCAB_TARG_SIZE, EMBEDDING_DIM, DEC_UNITS, BATCH_SIZE)\n",
    "hidden = tf.concat([sample_hidden[0], sample_hidden[1]], axis=-1)\n",
    "outputs , _, _ = decoder(tf.zeros(shape=(BATCH_SIZE, 1)), sample_sequence, hidden_state=hidden)\n",
    "print('Decoder outputs shape is', outputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef23a9af-4b9c-4231-97fc-3735f3fb927d",
   "metadata": {},
   "source": [
    "## Define the optimizer and Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "05785006-58c9-4266-8f56-c03a287c6423",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.losses import SparseCategoricalCrossentropy\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "93d2376b-4465-4096-9ca8-75de40136317",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = Adam()\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    # Make a mask that is 1 everywhere there is not a padding, and 0 for paddings\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "94b6a1a3-b823-47d6-99e8-fd328e8732e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, hidden):\n",
    "    loss = 0\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, (enc_h, enc_bh) = encoder(inp, hidden)\n",
    "        dec_hidden = tf.concat([enc_h, enc_bh], axis=-1)\n",
    "        dec_inputs = tf.expand_dims([targ_tokenizer.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "\n",
    "        for t in range(1, targ.shape[1]):\n",
    "            dec_outputs, dec_hidden, _ = decoder(dec_inputs, enc_output, dec_hidden)\n",
    "            computed_loss = loss_function(targ[:, t], dec_outputs)\n",
    "            loss = loss + computed_loss\n",
    "            dec_inputs = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "    batch_loss = loss / int(targ.shape[1])\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    grads = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(grads, variables))\n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1155c051-de7e-4739-b1d0-bfac63fc25a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1712125762.665667   31682 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step  1249 Total Loss 0.780\n",
      "Epoch   2\n",
      "Step  1249 Total Loss 0.668\n",
      "Epoch   3\n",
      "Step  1249 Total Loss 0.449\n",
      "Epoch   4\n",
      "Step  1249 Total Loss 0.315\n",
      "Epoch   5\n",
      "Step  1249 Total Loss 0.226\n",
      "Epoch   6\n",
      "Step  1249 Total Loss 0.167\n",
      "Epoch   7\n",
      "Step  1249 Total Loss 0.153\n",
      "Epoch   8\n",
      "Step  1249 Total Loss 0.121\n",
      "Epoch   9\n",
      "Step  1249 Total Loss 0.093\n",
      "Epoch  10\n",
      "Step  1249 Total Loss 0.086\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "for epoch in range(EPOCHS):\n",
    "    encoder.reset_hidden_states()\n",
    "    enc_hidden, enc_backward_hidden = encoder.get_hidden_states()\n",
    "    total_loss = 0\n",
    "\n",
    "    print(f'Epoch {epoch + 1:>3}')\n",
    "    for batch, (inp, tar) in enumerate(train_ds):\n",
    "        batch_loss = train_step(inp, tar, (enc_hidden, enc_backward_hidden))\n",
    "        total_loss += batch_loss.numpy()\n",
    "        print(f'\\rStep {batch:>5} Total Loss {batch_loss.numpy():>5.3f}', end='')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c37458-7d17-4b3f-b11c-e60a6d08188b",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "960f193d-7d12-4d0f-a1fa-b02c01389786",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(sentence):\n",
    "    attention_mtrx = np.zeros((MAX_TARGET_LENGTH, MAX_SRC_LENGTH))\n",
    "\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "    src = [src_tokenizer.word_index[x] for x in sentence.split(' ')]\n",
    "    src = pad_sequences([src], maxlen=MAX_SRC_LENGTH, padding='post')\n",
    "    src = tf.convert_to_tensor(src)\n",
    "\n",
    "    hidden = tf.zeros(shape=(1, ENC_UNITS), dtype=tf.float32)\n",
    "    enc_out, (enc_hidden, enc_backward_hidden) = encoder(src, hidden=(hidden, hidden))\n",
    "\n",
    "    dec_hidden = tf.concat([enc_hidden, enc_backward_hidden], axis=-1)\n",
    "    target_input = tf.expand_dims([targ_tokenizer.word_index['<start>']], 0)\n",
    "    result = ''\n",
    "\n",
    "    for t in range(MAX_TARGET_LENGTH):\n",
    "        predictions, dec_hidden, computed_attention = decoder(target_input, enc_out, dec_hidden)\n",
    "\n",
    "        computed_attention = tf.reshape(computed_attention, shape=(-1))\n",
    "        attention_mtrx[t] = computed_attention.numpy()\n",
    "\n",
    "        y = tf.argmax(predictions[0]).numpy()\n",
    "        result += targ_tokenizer.index_word[y] + ' '\n",
    "\n",
    "        if targ_tokenizer.word_index['<end>'] == y:\n",
    "            return result, sentence, attention_mtrx\n",
    "\n",
    "        target_input = tf.expand_dims([y], 0)\n",
    "\n",
    "    return result, sentence, attention_mtrx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1f708a4e-21c0-444a-86f9-f8d37660dd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for plotting the attention weights\n",
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "    fig = plt.figure(figsize=(5,5))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap='viridis')\n",
    "    \n",
    "    fontdict = {'fontsize': 14}\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2d0a1844-683e-41bb-ae17-f515091805ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "    result, sentence, attention_plot = predict(sentence)\n",
    "    print('Input: %s' % (sentence))\n",
    "    print('Predicted translation: {}'.format(result))   \n",
    "    \n",
    "    attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
    "    plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "31964b4f-3948-4239-8b79-0a142d73097d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> i love to speak in english <end>\n",
      "Predicted translation: me encanta hablar en ingles . <end> \n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "translate('i love to speak in English')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
