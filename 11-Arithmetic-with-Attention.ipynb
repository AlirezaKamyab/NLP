{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb6b5d38-6214-4d84-9473-e31bcab4cc57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-07 01:52:10.688454: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-07 01:52:10.688480: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-07 01:52:10.689280: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26aea5b4-59ef-4e4d-a8f1-25230ef84969",
   "metadata": {},
   "source": [
    "## Data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3ffba09-d78a-4c3e-9979-cce4e7778ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE_MAX_LENGTH = 13 + 2\n",
    "TARGET_MAX_LENGTH = 5 + 2\n",
    "EOS = '<EOS>'\n",
    "ops = ['+', '-', '*']\n",
    "exps = [['<E>', '<OP>', '<E>'], ['<N>', '<OP>', '<E>'], ['(', '<E>', '<OP>', '<E>', ')'], ['<N>']]\n",
    "\n",
    "\n",
    "def flatten(lst:list) -> list:\n",
    "    flattened = []\n",
    "    for item in lst:\n",
    "        if isinstance(item, (list, tuple)):\n",
    "            flattened.extend(item)\n",
    "        else:\n",
    "            flattened.append(item)\n",
    "    return flattened\n",
    "    \n",
    "\n",
    "def make_expression(expression, depth=0, min_depth=0, max_depth=3) -> list:\n",
    "    if len(expression) == 0: expression.append('<E>')\n",
    "    if '<E>' not in expression: return expression\n",
    "    if depth == max_depth:\n",
    "        for i in range(len(expression)):\n",
    "            if expression[i] == '<E>': expression[i] = '<N>'\n",
    "        return expression\n",
    "\n",
    "    while '<E>' in expression:\n",
    "        if depth > min_depth:\n",
    "            i = np.random.choice(len(exps))\n",
    "        else:\n",
    "            i = np.random.choice(len(exps[:-1]))\n",
    "        ei = expression.index('<E>')\n",
    "        expression[ei] = exps[i]\n",
    "    expression = flatten(expression)\n",
    "    return make_expression(expression, depth=depth + 1, min_depth=min_depth, max_depth=max_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "563f3366-25f7-4c50-94a3-40a9b5e9df56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_single_data(low=0, high=10, min_depth=0, max_depth=2):\n",
    "    expression = []\n",
    "    expression = make_expression(expression, min_depth=min_depth, max_depth=max_depth)\n",
    "    for i in range(len(expression)):\n",
    "        if expression[i] == '<N>':\n",
    "            num = np.random.randint(low, high)\n",
    "            expression[i] = str(num)\n",
    "        elif expression[i] == '<OP>':\n",
    "            op = np.random.choice(ops)\n",
    "            expression[i] = op\n",
    "    x = ''.join(expression)\n",
    "    y = str(eval(x))\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "451b790f-6622-4050-b6a2-6ae2a51c14a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(num_samples=1000, low=0, high=10, min_depth=1, max_depth=2):\n",
    "    source = []\n",
    "    target = []\n",
    "    cnt = 0\n",
    "    while cnt < num_samples:\n",
    "        try:\n",
    "            xi, yi = generate_single_data(low, high, min_depth=min_depth, max_depth=max_depth)\n",
    "            source.append(xi)\n",
    "            target.append(yi)\n",
    "            cnt += 1\n",
    "            if cnt % 1000 == 0:\n",
    "                print(f'\\rCount: {cnt:>9}', end='')\n",
    "        except:\n",
    "            pass\n",
    "    print()\n",
    "    return source, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "753a6c40-d248-496d-9a5d-3ec7a38a6739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count:     50000\n",
      "Count:     50000\n",
      "13 is the maximum length for source\n",
      "4 is the maximum length for target\n"
     ]
    }
   ],
   "source": [
    "source1, target1 = generate_data(num_samples=50_000, low=1, min_depth=1, max_depth=2)\n",
    "source2, target2 = generate_data(num_samples=50_000, low=1, min_depth=1, max_depth=1)\n",
    "\n",
    "source = np.concatenate([source1, source2], axis=-1)\n",
    "target = np.concatenate([target1, target2], axis=-1)\n",
    "\n",
    "idx = [x for x in range(len(source))]\n",
    "np.random.shuffle(idx)\n",
    "\n",
    "source = source[idx]\n",
    "target = target[idx]\n",
    "\n",
    "\n",
    "max_source_generated = 0\n",
    "max_target_generated = 0\n",
    "for xi, yi in zip(source, target):\n",
    "    max_source_generated = max(len(xi.strip()), max_source_generated)\n",
    "    max_target_generated = max(len(yi.strip()), max_target_generated)\n",
    "\n",
    "print(max_source_generated, 'is the maximum length for source')\n",
    "print(max_target_generated, 'is the maximum length for target')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378b1245-86a4-4393-8f4d-8b5a2b6b833d",
   "metadata": {},
   "source": [
    "## Prepration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c342da3e-3f08-4b99-8a59-7e96fcd98889",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94ac7e3f-cf8a-4adc-a54c-9ce629ef44ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "START_TOKEN = 's'\n",
    "END_TOKEN = 'e'\n",
    "\n",
    "def preprocess(source, target):\n",
    "    out_source = []\n",
    "    out_target = []\n",
    "    for i in range(len(source)):\n",
    "        out_source.append(f\"{START_TOKEN}{source[i]}{END_TOKEN}\")\n",
    "        out_target.append(f\"{START_TOKEN}{target[i]}{END_TOKEN}\")\n",
    "    return out_source, out_target\n",
    "\n",
    "\n",
    "def tokenize(data, padding='pre'):\n",
    "    tokenizer = Tokenizer(filters='', char_level=True)\n",
    "    tokenizer.fit_on_texts(data)\n",
    "    tokenized = tokenizer.texts_to_sequences(data)\n",
    "    padded = pad_sequences(tokenized, maxlen=SOURCE_MAX_LENGTH, padding=padding)\n",
    "    return padded, tokenizer\n",
    "\n",
    "out_source, out_target = preprocess(source, target)\n",
    "source, source_tokenizer = tokenize(out_source, padding='pre')\n",
    "target, target_tokenizer = tokenize(out_target, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c06a90e7-c323-48bb-8963-69e06afae94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "SHUFFLE_BUFFER = 1000\n",
    "BATCH_SIZE = 64\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "VALID_SIZE = int(len(source) * 0.2)\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((source, target))\n",
    "test_ds = dataset.take(VALID_SIZE)\n",
    "train_ds = dataset.skip(VALID_SIZE)\n",
    "\n",
    "train_ds = train_ds.shuffle(SHUFFLE_BUFFER)\n",
    "train_ds = train_ds.batch(BATCH_SIZE, drop_remainder=True, num_parallel_calls=AUTOTUNE)\n",
    "train_ds = train_ds.prefetch(AUTOTUNE)\n",
    "\n",
    "test_ds = test_ds.batch(BATCH_SIZE, drop_remainder=True, num_parallel_calls=True)\n",
    "test_ds = test_ds.prefetch(AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cacb0194-97bb-473b-806a-efc6b3517a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6+7+2-8) = 7\n",
      "2+6 = 8\n",
      "9-9 = 0\n",
      "4-(9*6) = -50\n",
      "7+8 = 15\n",
      "2*1 = 2\n",
      "7+5 = 12\n",
      "5+1 = 6\n",
      "9+1 = 10\n",
      "(6+3) = 9\n",
      "(8+6-4*1) = 10\n",
      "(5*9) = 45\n",
      "7*4 = 28\n",
      "(4+5) = 9\n",
      "9*6 = 54\n",
      "3-5*7 = -32\n",
      "9+1 = 10\n",
      "(1-3)-7-9 = -18\n",
      "(7+8+(9*2)) = 33\n",
      "6*9 = 54\n",
      "(2*2-8+3) = -1\n",
      "3*1 = 3\n",
      "6*6+6 = 42\n",
      "7+8*4 = 39\n",
      "4+6+7*8 = 66\n",
      "5*5 = 25\n",
      "((2+9)-1+2) = 12\n",
      "7*7 = 49\n",
      "8+8 = 16\n",
      "(3-7) = -4\n",
      "5*3 = 15\n",
      "7*7 = 49\n",
      "9+5+7+1 = 22\n",
      "(8*9) = 72\n",
      "2+9 = 11\n",
      "((8+7)-6-6) = 3\n",
      "4+1 = 5\n",
      "7+4 = 11\n",
      "(2*2) = 4\n",
      "(7-1)+(4+7) = 17\n",
      "7+4+2 = 13\n",
      "4*2+6 = 14\n",
      "(6*9-6*1) = 48\n",
      "(4*3) = 12\n",
      "(7-5+8+6) = 16\n",
      "4*4 = 16\n",
      "(1-9) = -8\n",
      "(5+8-4*5) = -7\n",
      "9-4 = 5\n",
      "5*4 = 20\n",
      "1*3 = 3\n",
      "4*9 = 36\n",
      "5+1*9 = 14\n",
      "4-2+2 = 4\n",
      "(8-6*6-1) = -29\n",
      "9+1*2-4 = 7\n",
      "4-5+2+2 = 3\n",
      "5+1+5 = 11\n",
      "5-6 = -1\n",
      "4-9+4-6 = -7\n",
      "3*1 = 3\n",
      "6*8 = 48\n",
      "(1+5) = 6\n",
      "2+1 = 3\n"
     ]
    }
   ],
   "source": [
    "for s, t in train_ds.take(1):\n",
    "    for i in range(len(s)):\n",
    "        ex_s = s[i]\n",
    "        ex_t = t[i]\n",
    "        ex_s = ''.join([source_tokenizer.index_word[x] for x in ex_s.numpy() if x != 0])\n",
    "        ex_t = ''.join([target_tokenizer.index_word[x] for x in ex_t.numpy() if x != 0])\n",
    "        print(ex_s[1:-1], \"=\", ex_t[1:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658ba558-5200-4345-8815-bc4318d82294",
   "metadata": {},
   "source": [
    "## Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9826caf-081b-47ba-88a4-827f17c14dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Layer, LSTM, Bidirectional, Activation, Embedding, Dense\n",
    "from keras.losses import SparseCategoricalCrossentropy\n",
    "from keras.optimizers import Adam\n",
    "from keras.metrics import Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "094a4dec-71bd-4e4a-a31c-d1d9d160ed74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(Model):\n",
    "    def __init__(self, units, vocab_size, embedding_dim, batch_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.units = units\n",
    "        self.vocab_size = vocab_size\n",
    "        self.batch_size = batch_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        self.embedding = Embedding(input_dim=vocab_size, output_dim=embedding_dim)\n",
    "        self.lstm = Bidirectional(LSTM(units=units, return_sequences=True, return_state=True))\n",
    "\n",
    "\n",
    "    def initialize_hidden_states(self):\n",
    "        h = tf.zeros((self.batch_size, self.units), dtype='float32')\n",
    "        c = tf.zeros((self.batch_size, self.units), dtype='float32')\n",
    "        bh = tf.zeros((self.batch_size, self.units), dtype='float32')\n",
    "        bc = tf.zeros((self.batch_size, self.units), dtype='float32')\n",
    "        return h, c, bh, bc\n",
    "\n",
    "\n",
    "    def call(self, inputs, hidden):\n",
    "        x = self.embedding(inputs)\n",
    "        x, h, c, bh, bc = self.lstm(x)\n",
    "        h = tf.concat([h, bh], axis=-1)\n",
    "        c = tf.concat([c, bc], axis=-1)\n",
    "        return x, h, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8714193-d954-4ff6-a4ff-e5708b1ba810",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.units = units\n",
    "\n",
    "        self.Wa = Dense(units=units)\n",
    "        self.Ua = Dense(units=units)\n",
    "        self.V = Dense(1)\n",
    "\n",
    "\n",
    "    def call(self, encoder_outputs, decoder_states):\n",
    "        h, c = decoder_states\n",
    "        \n",
    "        h = tf.concat([h, c], axis=-1)\n",
    "        h = tf.expand_dims(h, 1)\n",
    "\n",
    "        a = tf.nn.tanh(self.Wa(h) + self.Ua(encoder_outputs))\n",
    "        a = self.V(a)\n",
    "\n",
    "        attention_weights = tf.nn.softmax(a, axis=1)\n",
    "        context = attention_weights * encoder_outputs\n",
    "        context = tf.reduce_sum(context, axis=1)\n",
    "\n",
    "        return context, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "300fe845-d861-41d8-9d9e-1a1ba7202dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(Model):\n",
    "    def __init__(self, units, attention_units, vocab_size, embedding_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.units = units\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.attention_units = attention_units\n",
    "\n",
    "        self.embedding = Embedding(input_dim=vocab_size, output_dim=embedding_dim)\n",
    "        self.lstm = LSTM(units=units, return_state=True)\n",
    "        self.attention = BahdanauAttention(units=attention_units)\n",
    "        self.output_layer = Dense(vocab_size)\n",
    "\n",
    "\n",
    "    def call(self, inputs, encoder_outputs, hidden):\n",
    "        embeddings = self.embedding(inputs)\n",
    "        context, attention_weights = self.attention(encoder_outputs, hidden)\n",
    "\n",
    "        context = tf.expand_dims(context, 1)\n",
    "        concat = tf.concat([embeddings, context], axis=-1)\n",
    "\n",
    "        output, h, c = self.lstm(concat, initial_state=hidden)\n",
    "        output = self.output_layer(output)\n",
    "\n",
    "        return output, (h, c), attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5783d2ba-98ea-437c-bb09-a230d77ee4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE_VOCAB_SIZE = len(source_tokenizer.word_index) + 1\n",
    "TARGET_VOCAB_SIZE = len(target_tokenizer.word_index) + 1\n",
    "ENC_UNITS = 256\n",
    "DEC_UNITS = 512\n",
    "ATTENTION_UNITS = 10\n",
    "EMBEDDING_DIM = 50\n",
    "\n",
    "encoder = Encoder(ENC_UNITS, SOURCE_VOCAB_SIZE, EMBEDDING_DIM, BATCH_SIZE)\n",
    "decoder = Decoder(DEC_UNITS, ATTENTION_UNITS, TARGET_VOCAB_SIZE, EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ddb0e016-8ce6-4914-8d9b-a5950136199b",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam()\n",
    "loss_object = SparseCategoricalCrossentropy(from_logits=True)\n",
    "train_accuracy = Accuracy()\n",
    "valid_accuracy = Accuracy()\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    candidate_loss = loss_object(real, pred)\n",
    "    mask = tf.cast(mask, dtype='float32')\n",
    "    loss = candidate_loss * mask\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9b7df182-c060-4073-8ab1-4fc999f8ef01",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def training_step(source, target, enc_hidden):\n",
    "    loss = 0\n",
    "    with tf.GradientTape() as tape:\n",
    "        encoder_outputs, h, c = encoder(source, enc_hidden)\n",
    "        decoder_hidden = (h, c)\n",
    "\n",
    "        dec_inp = tf.expand_dims([target_tokenizer.word_index[START_TOKEN]] * BATCH_SIZE, 1)\n",
    "        for i in range(1, TARGET_MAX_LENGTH):\n",
    "            decoder_outputs, decoder_hidden, _ = decoder(dec_inp, encoder_outputs, decoder_hidden)\n",
    "            loss += loss_function(target[:, i], decoder_outputs)\n",
    "            decoder_outputs = tf.argmax(decoder_outputs, axis=-1)\n",
    "            \n",
    "            train_accuracy.update_state(target[:, i], decoder_outputs)\n",
    "            dec_inp = tf.expand_dims(target[:, i], 1)\n",
    "\n",
    "        batch_loss = loss / int(targ.shape[1])\n",
    "        variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "        grads = tape.gradient(loss, variables)\n",
    "        optimizer.apply_gradients(zip(grads, variables))\n",
    "        return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8f37c543-dfc6-4e7f-a28d-8cd96c431821",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def validation_step(source, target, enc_hidden, teacher_forcing=False):\n",
    "    mean_loss = 0\n",
    "    encoder_outputs, h, c = encoder(source, enc_hidden)\n",
    "    decoder_hidden = (h, c)\n",
    "\n",
    "    dec_inp = tf.expand_dims([target_tokenizer.word_index[START_TOKEN]] * BATCH_SIZE, 1)\n",
    "    for i in range(1, TARGET_MAX_LENGTH):\n",
    "        decoder_outputs, decoder_hidden, _ = decoder(dec_inp, encoder_outputs, decoder_hidden)\n",
    "        loss = loss_function(target[:, i], decoder_outputs)\n",
    "        mean_loss = mean_loss + loss\n",
    "\n",
    "        decoder_outputs = tf.argmax(decoder_outputs, axis=-1)\n",
    "        valid_accuracy.update_state(target[:, i], decoder_outputs)\n",
    "        if not teacher_forcing:\n",
    "            dec_inp = tf.expand_dims(decoder_outputs, 1)\n",
    "        else:\n",
    "            enc_inp = tf.expand_dims(target[:, i], 1)\n",
    "\n",
    "    mean_loss = mean_loss / int(target.shape[1])\n",
    "    return mean_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3d816b67-d438-4991-ab2a-6d0459393a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch     1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1712442152.515754   66801 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step  1250\tloss 0.174\taccuracy 0.836\n",
      "Step  312\tval_loss 0.141\tval_acc 0.878\n",
      "Epoch     2\n",
      "Step  1250\tloss 0.082\taccuracy 0.918\n",
      "Step  312\tval_loss 0.113\tval_acc 0.905\n",
      "Epoch     3\n",
      "Step  1250\tloss 0.061\taccuracy 0.937\n",
      "Step  312\tval_loss 0.111\tval_acc 0.909\n",
      "Epoch     4\n",
      "Step  1250\tloss 0.051\taccuracy 0.947\n",
      "Step  312\tval_loss 0.081\tval_acc 0.932\n",
      "Epoch     5\n",
      "Step  1250\tloss 0.044\taccuracy 0.953\n",
      "Step  312\tval_loss 0.066\tval_acc 0.944\n",
      "Epoch     6\n",
      "Step  1250\tloss 0.040\taccuracy 0.958\n",
      "Step  312\tval_loss 0.073\tval_acc 0.942\n",
      "Epoch     7\n",
      "Step  1250\tloss 0.036\taccuracy 0.961\n",
      "Step  312\tval_loss 0.054\tval_acc 0.958\n",
      "Epoch     8\n",
      "Step  1250\tloss 0.031\taccuracy 0.967\n",
      "Step  312\tval_loss 0.063\tval_acc 0.952\n",
      "Epoch     9\n",
      "Step  1250\tloss 0.025\taccuracy 0.973\n",
      "Step  312\tval_loss 0.041\tval_acc 0.970\n",
      "Epoch    10\n",
      "Step  1250\tloss 0.020\taccuracy 0.978\n",
      "Step  312\tval_loss 0.048\tval_acc 0.967\n",
      "Epoch    11\n",
      "Step  1250\tloss 0.018\taccuracy 0.981\n",
      "Step  312\tval_loss 0.039\tval_acc 0.972\n",
      "Epoch    12\n",
      "Step  1250\tloss 0.016\taccuracy 0.983\n",
      "Step  312\tval_loss 0.064\tval_acc 0.959\n",
      "Epoch    13\n",
      "Step  1250\tloss 0.014\taccuracy 0.985\n",
      "Step  312\tval_loss 0.039\tval_acc 0.974\n",
      "Epoch    14\n",
      "Step  1250\tloss 0.012\taccuracy 0.987\n",
      "Step  312\tval_loss 0.042\tval_acc 0.972\n",
      "Epoch    15\n",
      "Step  1250\tloss 0.012\taccuracy 0.988\n",
      "Step  312\tval_loss 0.044\tval_acc 0.972\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 15\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    encoder_hidden = encoder.initialize_hidden_states()\n",
    "    train_accuracy.reset_state()\n",
    "    valid_accuracy.reset_state()\n",
    "    \n",
    "    print(f'Epoch {epoch + 1:>5}')\n",
    "\n",
    "    mean_loss = 0\n",
    "    for step, (src, targ) in enumerate(train_ds):\n",
    "        loss = training_step(src, targ, encoder_hidden)\n",
    "        mean_loss = mean_loss + (1 / (step + 1)) * (loss - mean_loss)\n",
    "        train_acc = train_accuracy.result()\n",
    "        print(f'\\rStep {step + 1:>5}\\tloss {mean_loss:>4.3f}\\taccuracy {train_acc:2.3f}', end='')\n",
    "    print()\n",
    "\n",
    "    mean_val_loss = 0\n",
    "    for step, (src, targ) in enumerate(test_ds):\n",
    "        loss = validation_step(src, targ, encoder_hidden, teacher_forcing=False)\n",
    "        mean_val_loss = mean_val_loss + (1 / (step + 1)) * (loss - mean_val_loss)\n",
    "        val_acc = valid_accuracy.result()\n",
    "        print(f'\\rStep {step + 1:>4}\\tval_loss {mean_val_loss:>4.3f}\\tval_acc {val_acc:2.3f}', end='')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0590b0d1-ec97-4f47-a351-c426a6a1ce45",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "06f41c75-b41d-4611-89df-c9e73d04ad6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(source_inp):\n",
    "    source_inp = START_TOKEN.lower() + source_inp + END_TOKEN.lower()\n",
    "    inp = source_tokenizer.texts_to_sequences([source_inp])\n",
    "    inp = tf.convert_to_tensor(inp)\n",
    "    padded = pad_sequences(inp, maxlen=SOURCE_MAX_LENGTH, padding='pre')\n",
    "    hidden = encoder.initialize_hidden_states()\n",
    "    encoded_outputs, h, c = encoder(padded, hidden)\n",
    "    decoder_hidden = (h, c)\n",
    "\n",
    "    dec_inp = tf.expand_dims([target_tokenizer.word_index[START_TOKEN.lower()]] * 1, 1)\n",
    "    outputs = []\n",
    "    for i in range(1, TARGET_MAX_LENGTH):\n",
    "        decoder_outputs, decoder_hidden, attention_weights = decoder(dec_inp, encoded_outputs, decoder_hidden)\n",
    "        decoder_outputs = tf.argmax(decoder_outputs, axis=-1)\n",
    "        outputs.append(decoder_outputs)\n",
    "        dec_inp = tf.expand_dims(decoder_outputs, 1)\n",
    "    outputs = [target_tokenizer.index_word[x.numpy()[0]] for x in outputs if x != 0]\n",
    "    return ''.join(outputs[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1e62330b-b12d-4076-b123-30d2cdf80b29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Easy Ones\n",
      "5-3                            =        2 and predicted        2\n",
      "4+8                            =       12 and predicted       12\n",
      "(3-5)                          =       -2 and predicted       -2\n",
      "8-1                            =        7 and predicted        7\n",
      "4-3                            =        1 and predicted        1\n",
      "8-9                            =       -1 and predicted       -1\n",
      "8*7                            =       56 and predicted       56\n",
      "1+6                            =        7 and predicted        7\n",
      "7+8                            =       15 and predicted       15\n",
      "2*6                            =       12 and predicted       12\n",
      "8+9                            =       17 and predicted       17\n",
      "9*3                            =       27 and predicted       27\n",
      "1*5                            =        5 and predicted        5\n",
      "4*6                            =       24 and predicted       24\n",
      "(7+7)                          =       14 and predicted       14\n",
      "1*5                            =        5 and predicted        5\n",
      "8+5                            =       13 and predicted       13\n",
      "7*3                            =       21 and predicted       21\n",
      "8*4                            =       32 and predicted       32\n",
      "1+9                            =       10 and predicted       10\n",
      "\n",
      "Difficult ones\n",
      "9-(5*4)                        =      -11 and predicted      -11\n",
      "5*1-7                          =       -2 and predicted       -2\n",
      "6-5*8                          =      -34 and predicted      -32\n",
      "3-9*(8-7)                      =       -6 and predicted       -6\n",
      "(5*4+(9*6))                    =       74 and predicted       74\n",
      "1*2*(4+9)                      =       26 and predicted       26\n",
      "8-(5-3)                        =        6 and predicted        6\n",
      "7*7+6*3                        =       67 and predicted       69\n",
      "7+8-5                          =       10 and predicted       10\n",
      "2-8-6                          =      -12 and predicted      -12\n",
      "(9-5-(1+9))                    =       -6 and predicted       -6\n",
      "1-8*9-6                        =      -77 and predicted      -71\n",
      "((1+8)-(8+7))                  =       -6 and predicted       -6\n",
      "(8*1-1*3)                      =        5 and predicted        5\n",
      "(6-7)*8-2                      =      -10 and predicted      -10\n",
      "1*(3*9)                        =       27 and predicted       27\n",
      "((2+7)-(8*3))                  =      -15 and predicted      -15\n",
      "(7+8)+(3-1)                    =       17 and predicted       17\n",
      "7+5-7                          =        5 and predicted        5\n",
      "6-6*9                          =      -48 and predicted      -46\n"
     ]
    }
   ],
   "source": [
    "print('Easy Ones')\n",
    "for i in range(20):\n",
    "    test_source, test_target = generate_single_data(low=1, min_depth=1, max_depth=1)\n",
    "    pred = evaluate(test_source)\n",
    "    print(f'{test_source:<30} = {test_target:>8} and predicted {pred:>8}')\n",
    "\n",
    "print('\\nDifficult ones')\n",
    "for i in range(20):\n",
    "    test_source, test_target = generate_single_data(low=1, min_depth=1, max_depth=2)\n",
    "    pred = evaluate(test_source)\n",
    "    print(f'{test_source:<30} = {test_target:>8} and predicted {pred:>8}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
